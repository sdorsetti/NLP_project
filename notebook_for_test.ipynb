{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tokyo_2020_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import logging \n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "import re \n",
    "\n",
    "def remove_hashtags(tokens):\n",
    "    tokens = filter(lambda x: \"#\" not in x, tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_hastags_sign(tokens):\n",
    "    tokens = map(lambda x: x.replace('#',\"\"), tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_url(tokens):\n",
    "    tokens = filter(lambda x: \"http\" not in x, tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_html(tokens):\n",
    "    tokens = filter(lambda x: x[0]+x[-1] != '<>', tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_mentions(tokens):\n",
    "    tokens = filter(lambda x: \"@\" not in x, tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def detect_language(text):\n",
    "  try: \n",
    "    return detect(text)\n",
    "  except: \n",
    "    print(\"unreconnized character\")\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor():\n",
    "    def __init__(self, data:pd.DataFrame, column: str):\n",
    "        self.column = column\n",
    "        self.data = data\n",
    "        self.corpus = self.data[column].astype(str).array\n",
    "    @property\n",
    "    def preprocess(self,f_hastags = remove_hashtags, threshold = 50, url = True, html = True, hashtags=True, mentions = True):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            url (bool, optional): _description_. Defaults to True.\n",
    "            html (bool, optional): _description_. Defaults to True.\n",
    "            hashtags (bool, optional): _description_. Defaults to True.\n",
    "            mentions (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokenized_sentences = []\n",
    "        logging.info(\"tokenizing\")\n",
    "        for sentence in tqdm(self.corpus):\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            if url :\n",
    "                tokens = remove_url(tokens)\n",
    "            if html: \n",
    "                tokens = remove_html(tokens)\n",
    "            if hashtags: \n",
    "                tokens = f_hastags(tokens)\n",
    "            if mentions: \n",
    "                tokens = remove_mentions(tokens)\n",
    "            tokens = list(map(lambda x: x.lower(), tokens))\n",
    "            tokenized_sentences.append(tokens)\n",
    "        logging.info(\"Phrasing\")\n",
    "        phrases = Phrases(tokenized_sentences, threshold=threshold)\n",
    "        phraser = Phraser(phrases)\n",
    "        clean_corpus = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            clean_corpus.append(phraser[sentence])\n",
    "        return [\" \".join(i) for i in clean_corpus]\n",
    "    \n",
    "    def get_df(self, to_csv, language='en', detect_language_=True, remove_emojis=False):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            language (str, optional): _description_. Defaults to 'en'.\n",
    "            detect_language_ (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(\"****PREPROCESSING COLUMN {} OF TWEETS DF\".format(self.column))\n",
    "        df = self.data.copy()\n",
    "        if remove_emojis : \n",
    "            df[self.column] = df[self.column].apply(lambda x : str(remove_emoji(x)))\n",
    "\n",
    "        df[\"cleaned_{}\".format(self.column)] = self.preprocess\n",
    "        if detect_language_:\n",
    "            logging.info(\"language detection\")\n",
    "            tqdm.pandas()\n",
    "            df[\"language\"] = df[\"cleaned_{}\".format(self.column)].progress_apply(lambda x : detect_language(x))\n",
    "            df = df[df['language'] == language]\n",
    "            shape = df.shape[0]\n",
    "            df = df.drop([\"language\"],axis=1)\n",
    "            logging.info(\"{} % of tweets in {}\".format(df.shape[0]/shape*100, language))\n",
    "        if to_csv: \n",
    "            df.to_csv(structure_dict[\"output_path\"] + \"preprocessed_df.csv\", index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'label'\n",
    "csv_path = \"C:/Users/Stanislasd’Orsetti/NLP_project/data/labellized_hashtags_df.csv\"\n",
    "df = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "11737    0\n",
       "11738    0\n",
       "11739    0\n",
       "11740    0\n",
       "11741    0\n",
       "Name: Badminton, Length: 11742, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Badminton\"].loc[:].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'label'\n",
    "csv_path = \"C:/Users/Stanislasd’Orsetti/NLP_project/data/labellized_hashtags_df.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "df_dummies = df[label_column].apply(lambda x : ast.literal_eval(x)).str.join('|').str.get_dummies()\n",
    "list_columns =df_dummies.columns.tolist()\n",
    "df_expl = df.copy()\n",
    "df= pd.concat([df_expl, df_dummies], axis=1 )\n",
    "\n",
    "for elem in list_columns : \n",
    "    df[elem].loc[df[elem].isnull()] = df[elem].loc[df[elem].isnull()].apply(lambda x: 0)\n",
    "    df[elem].loc[:]=df[elem].loc[:].apply(lambda x: int(x))\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "LABEL_COLUMNS = df_dummies.columns.tolist()[:]\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "\n",
    "train_with = train_df[train_df[LABEL_COLUMNS].sum(axis=1) > 0]\n",
    "train_without= train_df[train_df[LABEL_COLUMNS].sum(axis=1) == 0]\n",
    "\n",
    "train_df = pd.concat(\n",
    "    [train_with.sample(600),\n",
    "    train_without]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stanislasd’Orsetti\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-4752d73831b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelisation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabuilder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructure_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelisation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLABEL_COLUMNS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHashtagTweetTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelisation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorch_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHashTagTweetDataModule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelisation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/Stanislasd’Orsetti/NLP_project\\hashtags\\modelisation\\model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#BUILD TRAINING DF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mval_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/Stanislasd’Orsetti/NLP_project\\hashtags\\modelisation\\databuilder.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(df, label_column)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mLABEL_COLUMNS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"__{converter.__name__}__\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"C:/Users/Stanislasd’Orsetti/NLP_project/\")\n",
    "\n",
    "from hashtags.modelisation.databuilder import create_dataset\n",
    "from config import structure_dict\n",
    "from hashtags.modelisation.model import LABEL_COLUMNS, HashtagTweetTagger\n",
    "from hashtags.modelisation.torch_dataset import HashTagTweetDataModule\n",
    "from hashtags.modelisation.utils import *\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BUILD TRAINING DF\n",
    "train_df, val_df = create_dataset(df,column)\n",
    "train_df = train_df.copy()\n",
    "val_df = val_df.copy()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "#TORCHDATA\n",
    "data_module = HashTagTweetDataModule(\n",
    "  train_df,\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "#MODEL\n",
    "model = HashtagTweetTagger(\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "#TRAINING\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"hashtags_tweets\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints\",\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "  accelerator=\"cpu\",\n",
    "  logger=logger,\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "  callbacks=[early_stopping_callback],\n",
    "  max_epochs=N_EPOCHS,\n",
    "  gpus=1,\n",
    "  progress_bar_refresh_rate=30,\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test()\n",
    "\n",
    "torch.save(model.state_dict(), \"hashtags/model.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb75eb01dedf8b1a2e8c97d0a50be717c6a0ce94af4f3c765f7d98ac8bc1ea43"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
